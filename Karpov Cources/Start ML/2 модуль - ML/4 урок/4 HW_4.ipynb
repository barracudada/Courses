{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем градиентный спуск для задачи поиска оптимальных коэффициентов в MSE регрессии!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Имеем 1000 объектов и 10 признаков у каждого (+таргет)!\n",
    "\n",
    "Обучим модель линейной регрессии:\n",
    "\n",
    "$$\n",
    "a(x) = \\beta_1 d_{1} + \\beta_2 d_{2} + \\beta_3 d_{3} + \\beta_4 d_{4} + \\beta_5 d_{5} + \\beta_6 d_{6} + \\beta_7 d_{7} + \\beta_8 d_{8} + \\beta_9 d_{9} + \\beta_{10} d_{10} + \\beta_0\n",
    "$$\n",
    "\n",
    "Которая минимизирует MSE:\n",
    "\n",
    "$$\n",
    "Q(a(X), Y) = \\sum_i^{1000} (a(x_i) - y_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим коэффициенты линейной регрессии с помощью библиотеки <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\"> **sklearn** </a>\n",
    "\n",
    "Отдельно выведем оценку свободного коэффициента  ($\\beta_0$ при $d_0 = 1$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "### Your code is here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь вам необходимо реализовать класс для оптимизации коэффициентов линейной регрессии МНК.\n",
    "Подразумевается, что на вход алгоритм будет принимать следующие параметры:\n",
    "\n",
    "- 2 pandas датафрейма **samples** и **targets**, содержащих матрицу объектов и ветор ответов соответственно\n",
    "- значение **learning rate**, который корректирует длину вектора-градиента (чтобы он не взорвался)\n",
    "- значение **threshold**'а для критерия останова (когда мы считаем, что мы сошлись к оптимуму)\n",
    "- параметр **copy**, который позволяет либо делать изменения in-place в датафрейме, подающимся в класс, если изменения матрицы объектов в принципе при обучении имеются. Или же копировать объект при инициализации класса и возвращать новый объект, если требуется.\n",
    "\n",
    "Он будет состоять из следующих важных компонент-методов:\n",
    "\n",
    "- **add_constant_feature**: добавляет колонку с названием *constant* из единичек к переданному датафрейму **samples**. Это позволяет оценить свободный коэффициент $\\beta_0$.\n",
    "\n",
    "- **calculate_mse_loss**: вычисляет при текущих весах **self.beta** значение среднеквадратической ошибки.\n",
    "\n",
    "- **calculate_gradient**: вычисляет при текущих весах вектор-градиент по функционалу.\n",
    "\n",
    "- **iteration**: производит итерацию градиентного спуска, то есть обновляет веса модели, в соответствии с установленным **learning_rate = $\\eta$**: $\\beta^{(n+1)} = \\beta^{(n)} - \\eta \\cdot \\nabla Q(\\beta^{(n)})$\n",
    "\n",
    "- **learn**: производит итерации обучения до того момента, пока не сработает критерий останова обучения. В этот раз критерием останова будет следующее событие: во время крайней итерации изменение в функционале качества модели составило значение меньшее, чем **self.threshold**. Иными словами, $|Q(\\beta^{(n)}) - Q(\\beta^{(n+1)})| < threshold$.\n",
    "\n",
    "P.S. установите в **__init__** аттрибут экземпляра с названием **iteration_loss_dict**, который будет устроен следующим образом: на каждой итерации мы будем добавлять в словарь пару ключ-значение, где ключем будет номер итерации $n$, а значением - среднеквадратическая ошибка в точке $\\beta^{(n)}$. Это пригодится нам в будущем для визуализации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hint: пример вычисления производной"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Q(a, X) = \\frac{1}{N}\\cdot\\sum_{i=1}^N (\\beta_1 \\cdot d_{i1} + ... + \\beta_n \\cdot d_{in} - y_i)^2\n",
    "$$\n",
    "\n",
    "Выше - минимизируемая функция. Она зависит от n переменных: $\\beta_1, ..., \\beta_n$. Вектор-градиент - матрица с одной строчкой, состоящей из производных 1го порядка по всем переменным.\n",
    "\n",
    "$$\n",
    "\\nabla Q(a, X) = (Q'_{\\beta_1} \\;\\;\\; Q'_{\\beta_2} \\;\\;\\; ... \\;\\;\\; Q'_{\\beta_{n-1}}  \\;\\;\\;  Q'_{\\beta_n})\n",
    "$$\n",
    "\n",
    "Пример вычисления производной по первой переменной:\n",
    "\n",
    "$$\n",
    "Q'_{\\beta_1} = \\frac{2}{N} \\cdot \\sum_{i=1}^N d_{i1} (\\beta_1 \\cdot d_{i1} + ... + \\beta_{n} \\cdot d_{in} - y_i)\n",
    "$$\n",
    "\n",
    "Скажем, для нашего датасета X, Y вычислим эту саму производную при начальных единичных коэффициентах $\\beta_{start} = (1 \\;\\;\\; 1 \\;\\;\\; ...)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим для каждого объекта в начале выражение из скобочек: \n",
    "$$\n",
    "\\beta_1 \\cdot d_{i1} + ... + \\beta_{n} \\cdot d_{in} - y_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Инициализируем точку для коэффициентов в виде вектора из единичек\n",
    "initial_betas = np.ones(X.shape[1])\n",
    "\n",
    "### Получим выражение выше для каждого объекта. \n",
    "### Для этого скалярно перемножим строчки из X на наши beta\n",
    "\n",
    "scalar_value = np.dot(X, initial_betas.reshape(-1, 1)).ravel()\n",
    "scalar_value = (scalar_value - Y).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь полученное значение для каждого объекта умножим на соответствующее значение признака $d_1$:\n",
    "\n",
    "$$\n",
    "d_{i1} \\cdot (\\beta_1 \\cdot d_{i1} + ... + \\beta_{n} \\cdot d_{in} - y_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Возьмем столбик со значениями 1 признака\n",
    "\n",
    "d_i1 = X.values[:, 0]\n",
    "\n",
    "### Умножим каждый объект на соответствующее значение признака\n",
    "scalar_value = scalar_value * d_i1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Наконец, умножим все на 2 и усреднимся, \n",
    "### чтобы получить значение производной по первому параметру\n",
    "\n",
    "2 * np.mean(scalar_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Эта логика поможем Вам при реализации класса!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learn(self)\n",
    "\n",
    "метод возвращает итоговую среднеквадратическую ошибку.\n",
    "метод итеративно вычисляет среднеквадратическую ошибку и вектор-градиент. номер итерации и MSE записываются в словарь *iteration_loss_dict*. критерий останова срабатывает тогда, когда абсолютное значение разницы двух последних MSE меньше *self.threshold*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescentMse:\n",
    "    \"\"\"\n",
    "    Базовый класс для реализации градиентного спуска в задаче линейной МНК регрессии \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, samples: pd.DataFrame, targets: pd.DataFrame,\n",
    "                 learning_rate: float = 1e-3, threshold = 1e-6, copy: bool = True):\n",
    "        \"\"\"\n",
    "        self.samples - матрица признаков\n",
    "        self.targets - вектор таргетов\n",
    "        self.beta - вектор из изначальными весами модели == коэффициентами бета (состоит из единиц)\n",
    "        self.learning_rate - параметр *learning_rate* для корректировки нормы градиента\n",
    "        self.threshold - величина, меньше которой изменение в loss-функции означает остановку градиентного спуска\n",
    "        iteration_loss_dict - словарь, который будет хранить номер итерации и соответствующую MSE\n",
    "        copy: копирование матрицы признаков или создание изменения in-place\n",
    "        \"\"\"\n",
    "        ### Your code is here\n",
    "        \n",
    "    def add_constant_feature(self):\n",
    "        \"\"\"\n",
    "        Метод для создания константной фичи в матрице объектов samples\n",
    "        Метод создает колонку с константным признаком (interсept) в матрице признаков.\n",
    "        Hint: так как количество признаков увеличилось на одну, не забудьте дополнить вектор с изначальными весами модели!\n",
    "        \"\"\"\n",
    "        ### Your code is here\n",
    "        \n",
    "    def calculate_mse_loss(self) -> float:\n",
    "        \"\"\"\n",
    "        Метод для расчета среднеквадратической ошибки\n",
    "        \n",
    "        :return: среднеквадратическая ошибка при текущих весах модели : float\n",
    "        \"\"\"\n",
    "        ### Your code is here\n",
    "\n",
    "    def calculate_gradient(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Метод для вычисления вектора-градиента\n",
    "        Метод возвращает вектор-градиент, содержащий производные по каждому признаку.\n",
    "        Сначала матрица признаков скалярно перемножается на вектор self.beta, и из каждой колонки\n",
    "        полученной матрицы вычитается вектор таргетов. Затем полученная матрица скалярно умножается на матрицу признаков.\n",
    "        Наконец, итоговая матрица умножается на 2 и усредняется по каждому признаку.\n",
    "        \n",
    "        :return: вектор-градиент, т.е. массив, содержащий соответствующее количество производных по каждой переменной : np.ndarray\n",
    "        \"\"\"\n",
    "        ### Your code is here\n",
    "    \n",
    "    \n",
    "    def iteration(self):\n",
    "        \"\"\"\n",
    "        Обновляем веса модели в соответствии с текущим вектором-градиентом\n",
    "        \"\"\"\n",
    "        ### Your code is here\n",
    "        \n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        Итеративное обучение весов модели до срабатывания критерия останова\n",
    "        Запись mse и номера итерации в iteration_loss_dict\n",
    "        \n",
    "        Описание алгоритма работы для изменения бет:\n",
    "            Фиксируем текущие beta -> start_betas\n",
    "            Делаем шаг градиентного спуска\n",
    "            Записываем новые beta -> new_betas\n",
    "            Пока |L(new_beta) - L(start_beta)| >= threshold:\n",
    "                Повторяем первые 3 шага\n",
    "                \n",
    "        Описание алгоритма работы для изменения функции потерь:\n",
    "            Фиксируем текущие mse -> previous_mse\n",
    "            Делаем шаг градиентного спуска\n",
    "            Записываем новые mse -> next_mse\n",
    "            Пока |(previous_mse) - (next_mse)| >= threshold:\n",
    "                Повторяем первые 3 шага\n",
    "        \"\"\"\n",
    "        ### Your code is here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим коэффициенты линейной модели с помощью реализованного нами градиентного спуска, не забыв добавить свободную переменную. Получились ли такие же коэффициенты, как и при использовании **LinearRegression** из **sklearn**? Если нет, то почему они отличаются, на Ваш взгляд, и сильно ли?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GD = GradientDescentMse(samples=X, targets=Y)\n",
    "GD.add_constant_feature()\n",
    "GD.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Веса модели при переменных d0, d1, ..., d10 равны соответственно: \\n\\n' + str(GD.beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуйте теперь изменить значения **learning_rate** и/или **threshold**. Например, установите длину шага $\\eta = 1$. Что произошло и почему такое возможно?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code is here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В машинном обучении зачастую исследуют так называемые **траектории обучения** (или **learning paths**). Это графики, показывающие, как во время обучения при каждой следующей итерации изменялось значение минимизируемого функционала. Постройте такие траектории для различных **learning rate**'ов и **threshold**'ов. Советуем использовать для этого разобранный на занятиях **add_subplot** метод. \n",
    "\n",
    "Возьмите следующие **threshold**'ы: 1e-2, 1e-3, 1e-4, 1e-5\n",
    "\n",
    "И следующие значения **learning rate**'а: 1e-1, 5e-2, 1e-2, 5e-3, 1e-3\n",
    "\n",
    "У вас должен получиться примерно такой график (см. ниже, значения среднеквадратической ошибки мы намеренно замазали оранжевыми квадратиками, чтобы не спойлерить вам результаты).\n",
    "\n",
    "Как и подобает хорошим Data Scientist'ам, не забывайте подписывать графики, оси, а так же делать элементы ваших визуализаций читаемыми и видимыми. Советуем пересмотреть методы и параметры форматирования из лекции.\n",
    "\n",
    "При какой комбинации **threshold** - **learning rate** из возможных предложенных выше, получается достигнуть меньшего значения нашей минимизируемой функции? Запишите каждой из значений в легенде на графиках.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Your code is here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
