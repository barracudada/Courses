{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39aad045",
   "metadata": {},
   "source": [
    "### Блок теоретических вопросов "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4240ae96",
   "metadata": {},
   "source": [
    "*Основная идея метода главных компонент заключается в*: \n",
    "\n",
    "1.\tОтборе m самых важных признаков согласно критерию максимизации дисперсии.\n",
    "2.\tВыкидывании каждого признака по отдельности, пока качество модели не вырастет.\n",
    "3.\tУдалении неинформативных признаков, веса которых зануляются при l1-регуляризации.\n",
    "4.\tНахождении m новых признаков, которые линейно выражаются через исходные.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Ответ: 4)** по построению метода это так. Каждый новый признак - линейная комбинация из предыдущих $z_j = \\sum_i^m w_{ji} \\cdot x_i$\n",
    "\n",
    "___________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bb7a7a",
   "metadata": {},
   "source": [
    "*Метод главных компонент подбирает веса для новых признаков решая задачу*: \n",
    "\n",
    "1.\tМаксимизации дисперсии объектов в изначальном признаковом пространстве.\n",
    "2.\tМаксимизации дисперсии объектов в новом признаковом пространстве.\n",
    "3.\tМинимизации дисперсии объектов в изначальном признаковом пространстве.\n",
    "4.\tМинимизации дисперсии объектов в новом признаковом пространстве.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Ответ: 2)** Такой подход позволяет как можно больше исходной информации *затянуть* в новое пространство\n",
    "\n",
    "___________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a1b359",
   "metadata": {},
   "source": [
    "*Метод главных компонент НЕ используется для задачи*: \n",
    "\n",
    "1.\tИзвлечения новых признаков из старых путем максимизации дисперсии.\n",
    "2.\tПроецирования данных из многомерного признакового пространства для визуализации данных.\n",
    "3.\tПредсказания целевой переменной: вектор, максимизирующий дисперсию исходных признаков, является хорошим предсказанием модели.\n",
    "4.\tВыделение нескольких наиболее информативных признаков для ускорения обучения моделей.\n",
    "\n",
    "\n",
    "\n",
    "**Ответ: 3)** Мы можем использовать полученные фичи для дообучения или переобучения моделей, но сам по себе МГК просто-напросто делает некоторый *feature extraction*\n",
    "\n",
    "___________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12f1fe0",
   "metadata": {},
   "source": [
    "*Корреляция между второй и третьей главными компонентами равна (Вопрос со звездочкой! Поэкспериментируйте в ноутбуке)*: \n",
    "\n",
    "1.\tИзвлечения новых признаков из старых путем максимизации дисперсии.\n",
    "2.\tПроецирования данных из многомерного признакового пространства для визуализации данных.\n",
    "3.\tПредсказания целевой переменной: вектор, максимизирующий дисперсию исходных признаков, является хорошим предсказанием модели.\n",
    "4.\tВыделение нескольких наиболее информативных признаков для ускорения обучения моделей.\n",
    "\n",
    "\n",
    "\n",
    "**Ответ: 4)** Геометрически можно показать, что, в действительности, при максимизации дисперсии новые признаки оказываются перпендикулярно растянуты относительно друг друга.\n",
    "___________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e82ca5",
   "metadata": {},
   "source": [
    "*Качество модели, построенной на основании исходных признаков всегда лучше, чем качество той же модели на основании главных компонент*: \n",
    "\n",
    "1.\tДа, если количество компонент меньше количества исходных признаков.\n",
    "2.\tДа, если количество компонент больше количества исходных признаков.\n",
    "3.\tНет, качество модели на основании главных компонент может быть лучше, если, например, изначальные данные были сильно зашумлены или вызывали проблему мультиколлинеарности.\n",
    "4.\tНет, качество исходной модели всегда лучше, чем качество модели на основании главных компонент, если имеет место проклятие размерности.\n",
    "\n",
    "\n",
    "\n",
    "**Ответ: 3)** Иногда, как и говорилось на лекции, в действительности МГК может свести нас в такое пространство, в котором при прочих равных (при тех же моделях) можно лучше отлавливать зависимости, чем в изначальном. Такое может быть, например, если мы сталкиваемся с мультиколлинеарностью и/или изначально имеем много \"плохих\" - шумовых - признаков.\n",
    "___________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088c4a17",
   "metadata": {},
   "source": [
    "*Почему функция для измерения «схожести» объектов в новом пространстве не осталось в прежней (гауссовской форме) в методе t-SNE*: \n",
    "\n",
    "1.\tВ новом пространстве тяжело учесть высокоразмерную сущность данных, поэтому расстояние должно получаться больше, чем это позволяет сделать гауссовская форма\n",
    "2.\tЭто сделано для того, чтобы оптимизировать дивергенцию Кульбака-Лейблера было проще\n",
    "3.\tЭмпирически выявлено, что добиться схожести распределений функций для измерения «схожести» нельзя, если они обе в гауссовской форме\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Ответ: 1)** Подробное пояснение приводить не будем (t-SNE и так нагружен страшными формулами и математикой), но интуиция содержится в самом ответе, и краткое пояснение приводилось в лекции.\n",
    "___________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc50124e",
   "metadata": {},
   "source": [
    "*Главное отличие t-SNE от PCA заключается в том, что*: \n",
    "\n",
    "1.\tt-SNE генерирует более информативные признаки.\n",
    "2.\tВ отличие от PCA, в t-SNE новые признаки имеют квадратичную зависимость от старых признаков.\n",
    "3.\tt-SNE обычно обучается быстрее PCA.\n",
    "4.\tПозволяет изображать даже сложные данные.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Ответ: 4)** В действительности, t-SNE умеет в нелинейные взаимосвязи между данными.\n",
    "___________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbb5065",
   "metadata": {},
   "source": [
    "### Блок практики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262e4433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9908eb26",
   "metadata": {},
   "source": [
    "### Загрузим уже обработанные в прошлом ДЗ данные!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9070905",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('processed_sigma.csv')\n",
    "df = df.drop(\"Trip_ID\", axis=1)\n",
    "\n",
    "X = df.drop(\"Surge_Pricing_Type\", axis=1)\n",
    "y = df[\"Surge_Pricing_Type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2590adab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Surge_Pricing_Type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad26278",
   "metadata": {},
   "source": [
    "### Principal component analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76433ce",
   "metadata": {},
   "source": [
    "**Задание 1.** Спроецируйте данные на двумерную плоскость, используя `PCA`. Другими словами, выделите первые две главные компоненты и изобразите данные в осях этих компонент. Не забудьте центрировать признаки перед применением метода. \n",
    "\n",
    "Какую долю изначальной дисперсии (информации) содержат/объясняют в себе обе компоненты? Для этого воспользуйтесь атрибутом `explained_variance_ratio_` у PCA-класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de818f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Центрируем данные\n",
    "\n",
    "X_centered = X - X.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecef8de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Выделим первые две главные компоненты\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_centered)\n",
    "X_pca = pd.DataFrame(X_pca, columns=['PCA_1', 'PCA_2'])\n",
    "X_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20dc451",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Изобразим точки в новом пространстве\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.title(\"Data projection on the two principal components.\")\n",
    "sns.scatterplot(X_pca['PCA_1'], X_pca['PCA_2']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f524f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Первая и вторая компоненты объясняют {pca.explained_variance_ratio_} долю дисперсии соответственно.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0761ddda",
   "metadata": {},
   "source": [
    "**Задание 2.** Изобразите heatmap корреляции изначальных признаков с новыми двумя главными компонентами. \\\n",
    "Какие признаки коррелируют с первой компонентой? Какие со второй?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca97fbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Попарные корреляции между новыми векторами и старой табличкой\n",
    "\n",
    "pc1_corr = X.corrwith(X_pca['PCA_1'])\n",
    "pc2_corr = X.corrwith(X_pca['PCA_2'])\n",
    "\n",
    "corrs = pd.concat((pc1_corr, pc2_corr), axis=1)\n",
    "corrs.columns = ['PCA_1', 'PCA_2']\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corrs, \n",
    "            xticklabels=corrs.columns,\n",
    "            yticklabels=corrs.index,\n",
    "            cmap='BrBG',\n",
    "            vmin=-1,\n",
    "            vmax=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9755a37b",
   "metadata": {},
   "source": [
    "--- Первая включает в себя в основном информацию по **Trip_Distance**, **Life_Style_Index**, **Var2**, **Var3**, **Confidence_Life_Style_Index_B**. Вторая же компонента формируется практически исключительно из **Var1**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a01f87c",
   "metadata": {},
   "source": [
    "**Задание 3.** Изобразите двумерную проекцию данных с метками классов. Выделите третью главную компоненту и аналогично нарисуйте трехмерную проекцию с метками классов. \\\n",
    "Какие признаки коррелируют с третьей компонентой?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8368a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Раскрасим точки в их классы\n",
    "\n",
    "PCA_df = np.concatenate((X_pca.values, y.values.reshape(-1, 1)), axis=1)\n",
    "PCA_df = pd.DataFrame(PCA_df, columns=['PCA_1', 'PCA_2', 'PricingType'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.title(\"Data projection on the two principal components.\")\n",
    "sns.scatterplot(data=PCA_df, x=\"PCA_1\", y=\"PCA_2\", hue=\"PricingType\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8798ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Теперь выделим три компоненты\n",
    "\n",
    "pca3 = PCA(n_components=3)\n",
    "\n",
    "X_pca3 = pca3.fit_transform(X_centered)\n",
    "X_pca3 = pd.DataFrame(X_pca3, columns=['PCA_1', 'PCA_2', 'PCA_3'])\n",
    "\n",
    "PCA_df3 = np.concatenate((X_pca3.values, y.values.reshape(-1, 1)), axis=1)\n",
    "PCA_df3 = pd.DataFrame(PCA_df3, columns=['PCA_1', 'PCA_2', 'PCA_3', 'PricingType'])\n",
    "PCA_df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd114852",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Изобразим, пользуясь Axes3D\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = Axes3D(fig, auto_add_to_figure=False)\n",
    "fig.add_axes(ax)\n",
    "\n",
    "plt.title(\"Data projection on the 3 principal components.\")\n",
    "ax.set_xlabel(\"PCA_1\")\n",
    "ax.set_ylabel(\"PCA_2\")\n",
    "ax.set_zlabel(\"PCA_3\")\n",
    "\n",
    "colors = PCA_df3['PricingType'].replace([1, 2, 3], ['green', 'red', 'blue'])\n",
    "\n",
    "ax.scatter(PCA_df3['PCA_1'], \n",
    "           PCA_df3['PCA_2'], \n",
    "           PCA_df3['PCA_3'], c=colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95345af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Найдем корреляции\n",
    "\n",
    "pc1_corr = X.corrwith(X_pca3['PCA_1'])\n",
    "pc2_corr = X.corrwith(X_pca3['PCA_2'])\n",
    "pc3_corr = X.corrwith(X_pca3['PCA_3'])\n",
    "\n",
    "corrs = pd.concat((pc1_corr, pc2_corr, pc3_corr), axis=1)\n",
    "corrs.columns = ['PCA_1', 'PCA_2', 'PCA_3']\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corrs, \n",
    "            xticklabels=corrs.columns,\n",
    "            yticklabels=corrs.index,\n",
    "            cmap='BrBG',\n",
    "            vmin=-1,\n",
    "            vmax=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e697e0d",
   "metadata": {},
   "source": [
    "--- Третья компонента содержит в себе информацию, по большей части, о переменных **Var2**, **Var3**, **Customer_Rating**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4c6670",
   "metadata": {},
   "source": [
    "Советуем еще и замерить попарные корреляции между главными компонентами! Это поможет в ответе на один из тестовых вопросов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992e2461",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca3.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a072efb",
   "metadata": {},
   "source": [
    "**Задание 4.** Обучите логистическую регрессию на первых двух компонентах и подберите гиперпараметры. Обучите логистическую регрессию на изначальных данных. Сравните метрику accuracy и f1-score на тестовой выборке в двух моделях.\n",
    "\n",
    "P.S. для удобства, скорости и нормализации данных (понадобится, так как захотим повалидироваться на параметрах регуляризации) можно построить конструкцию Pipeline c 3 следующими степами: StandardScaler -> PCA -> LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e9f919",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2022)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "### Разделим выборку\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_centered, y,\n",
    "                                                    test_size=0.2, \n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f00a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Построим пайплайн\n",
    "\n",
    "pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                 ('pca', PCA(n_components=2)),\n",
    "                 ('model', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d1a89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Подберем лучшие гиперпараметры по сетке\n",
    "\n",
    "param_grid = {'model__penalty': ['l1', 'l2', 'elasticnet'],\n",
    "              'model__C': [1e-5, 0.0001, 0.001, 0.01]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid.best_params_}\")\n",
    "print(f\"Best score ACC: {grid.best_score_}\")\n",
    "print(classification_report(grid.predict(X_test), y_test, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae8aa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Теперь без PCA в пайплайне\n",
    "\n",
    "pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                 ('model', LogisticRegression(solver='saga'))])\n",
    "\n",
    "param_grid = {'model__penalty': ['l1', 'l2', 'elasticnet'],\n",
    "              'model__C': [1e-5, 0.0001, 0.001, 0.01]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid.best_params_}\")\n",
    "print(f\"Best score ACC: {grid.best_score_}\")\n",
    "print(classification_report(grid.predict(X_test), y_test, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83d80e1",
   "metadata": {},
   "source": [
    "Кажется, достаточно сильно теряем в качестве! Может, добавить компонент?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a50e39b",
   "metadata": {},
   "source": [
    "**Задание 5.** Обучите модели логистической регресии на нескольких различных компонентах: [2, 4, 8, 16, 28]. Нарисуйте графики зависимости accuracy, f1-score от количества компонент. При обучении используйте l2-регуляризацию, гиперпараметр C подбирайте по сетке. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e68323",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Чтобы не заставлять вас писать похожий на предыдущий код,\n",
    "### Написали функцию, которая будет обучать модель для\n",
    "### произвольного числа n_components главных компонент.\n",
    "\n",
    "def train_pca_i(n_components, X_train, y_train):\n",
    "    pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                     ('pca', PCA(n_components=n_components)),\n",
    "                     ('model', LogisticRegression(penalty='l2', solver='saga'))])\n",
    "    \n",
    "    param_grid = {'model__C': [0.0001, 0.001, 0.01, 0.1]}\n",
    "    \n",
    "    grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    report = classification_report(grid.predict(X_test), y_test, output_dict=True)\n",
    "    return (report['accuracy'], \n",
    "            report['weighted avg']['f1-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a16d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "accuracy_dict, f1_score_dict = {}, {}\n",
    "components_list = [2, 4, 8, 16, 28]\n",
    "\n",
    "for n_components in components_list:\n",
    "    accuracy, f1_score = train_pca_i(n_components, X_train, y_train)\n",
    "    accuracy_dict[n_components] = accuracy\n",
    "    f1_score_dict[n_components] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fee9bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.plot(components_list, accuracy_dict.values(), label='Accuracy')\n",
    "plt.plot(components_list, f1_score_dict.values(), label='f1-score')\n",
    "\n",
    "plt.xlabel('n components')\n",
    "plt.legend()\n",
    "plt.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bb32cd",
   "metadata": {},
   "source": [
    "Обычно, когда решается какая-то высокоразмерная предсказательная задача, где тысячи признаков, функция качества PCA моделей похожа на параболу ветвями вниз в зависимости от количества компонент (для совсем маленьких и больших n_component качество маленькое, при этом есть где-то промежуточное значение).\n",
    "\n",
    "Как думаете, почему здесь у нас непрерывно растущие графики качества?\n",
    "\n",
    "Может, изначальные признаки были слишком хороши? -- вероятно!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b5a165",
   "metadata": {},
   "source": [
    "### t-SNE\n",
    "\n",
    "Напомним, что метод заключается в введении некоторых функций, основанных на расстоянии между признаками, которые измеряют близость объектов. Функций, как вы помните, две, и они заданы в исходном и новом пространствах соотвественно. Далее минимизируем дивергенцию Кульбака-Лейблера по координатaм в новом пространстве"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa315cb",
   "metadata": {},
   "source": [
    "**Задание 6.** Обучите t-sne c `n_components=2` на изначальных данных и визуализируйте результаты с учетом известных классов. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe189b78",
   "metadata": {},
   "source": [
    "P.S. если устанете ждать обучения TSNE, или упадет ошибка по типу oom, то можно сократить число объектов в датасете до, например, пары тысяч. Это и следующее задания никак проверяться системой не будут. Вам достаточно повторить упражнение с практики и получить какое-то более адекватное (по сравнению с PCA) изображение классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a74801b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Все по аналогии с PCA\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "X_tsne = tsne.fit_transform(X_centered)\n",
    "X_tsne = pd.DataFrame(X_tsne, columns=['t-SNE_1', 't-SNE_2'])\n",
    "X_tsne.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e424ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tsne['targ'] = y\n",
    "\n",
    "X_tsne['targ'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3fc809",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_size_inches(16, 10)\n",
    "    \n",
    "sns.scatterplot(data=X_tsne, x=\"t-SNE_1\", \n",
    "                y=\"t-SNE_2\", \n",
    "                hue=\"targ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56a3261",
   "metadata": {},
   "source": [
    "**Задание 7.** Обучите t-sne с `n_components=3` на изначальных данных и визуализируйте результаты с учетом известных классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845f5c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tsne_3d = TSNE(n_components=3).fit_transform(X_centered)\n",
    "\n",
    "X_tsne_3d\n",
    "\n",
    "### Преобразуем в pd.DataFrame\n",
    "\n",
    "X_tsne_3d = np.concatenate((X_tsne_3d, y.values.reshape(-1, 1)),\n",
    "                               axis=1)\n",
    "\n",
    "X_tsne_3d = pd.DataFrame(X_tsne_3d, columns=['t-SNE_1',\n",
    "                                             't-SNE_2',\n",
    "                                             't-SNE_3',\n",
    "                                             'targ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e19c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_size_inches(16, 10)\n",
    "\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "colors = X_tsne_3d['targ'].replace([1, 2, 3],\n",
    "                                      ['orange', 'green', 'red'])\n",
    "\n",
    "ax.scatter3D(X_tsne_3d['t-SNE_1'], \n",
    "             X_tsne_3d['t-SNE_2'],\n",
    "             X_tsne_3d['t-SNE_3'], \n",
    "             c=colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12db151b",
   "metadata": {},
   "source": [
    "Советуем обрезать датасет до маленького числа объектов (чтобы точки и их группки были хорошо различимы) и поиграться с гиперпараметрами класса Tsne! Обычно такая \"игра\" может привести к куда более красивым результатам по сравнению с теми, что получены с дефолтными настройками!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
